---
title: "Prediction Assignment Writeup (Practical Machine Learning)"
author: "Nandan Prasad"
date: "October 19, 2020"
output: html_document
---


# 1. Summary

This is a report for the prediction assignment (peer-graded assessment). My goal is to predict how 6 participants performed their exercises. The ML algorithm is applied to the available 20 test cases. 

# 2. Libraries

Loading required libraries.
```{r results='hide', message=FALSE}
library(corrplot)
library(rattle)
library(caret)
```

### 3. Load Data

Loading train and test datasets.
```{r results='hide', message=FALSE}
TrainSet <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header=TRUE)
dim(TrainSet)
TestSet <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), header=TRUE)
dim(TestSet)
```

### 4. Processing Data

```{r  message=FALSE}
# 70% of the data is train set and remaining 30% is assigned as test set.
set.seed(1209480)
inTrain <- createDataPartition(TrainSet$classe, p = 0.7, list = FALSE)
trainSet <- TrainSet[inTrain, ]
testSet <- TrainSet[-inTrain, ]
dim(trainSet)
dim(testSet)

# Removing near zero variance variables.
nearZeroVarianceVariables <- nearZeroVar(trainSet)
trainSet <- trainSet[, -nearZeroVarianceVariables]
testSet  <- testSet[, -nearZeroVarianceVariables]
dim(trainSet)
dim(testSet)

# Removing variables that contain most NA entries. Threshold is set at 90%.
NAvalueVariables <- sapply(trainSet, function(x) mean(is.na(x))) > 0.9
NAvalueTest <- sapply(testSet, function(x) mean(is.na(x))) > 0.9
trainSet <- trainSet[, NAvalueVariables==F]
testSet <- testSet[, NAvalueTest==F]
dim(trainSet)
dim(testSet)

# Removing identification variables 
trainSet <- trainSet[, -(1:5)]
testSet <- testSet[, -(1:5)]
dim(trainSet)
dim(testSet)
```
### 5. Data Analysis 

## Check correlation among variables

```{r results='hide', message=FALSE}
correlation <- cor(trainSet[, -54])
corrplot(correlation, method="circle")
# The circles with dark colors show highly correlated variables in the graph above. Correlations do not seem to give any analysis points as they are very less.
```

## Classification Tree (CT)
```{r message=FALSE}
trControl <- trainControl(method="cv", number=5)
CTmodel <- train(classe~., , method="rpart", data=trainSet, trControl=trControl)
fancyRpartPlot(CTmodel$finalModel)
predictTrain <- predict(CTmodel,newdata=testData)
confusionMatrixCT <- confusionMatrix(testData$classe,predict_train)

# Displaying confusion matrix and accuracy
confusionMatrixCT$table
confusionMatrixCT$overall[1]
```

## Random Forest (RF)
```{r message=FALSE}
random_forest <- trainControl(method="cv", number=3, verboseIter=FALSE)
RFmodel <- train(classe ~ ., data=trainSet, method="rf", trControl=random_forest)
RFmodel$finalModel
plot(RFmodel,main="Accuracy of RF model")
predictTrain <- predict(RFmodel,newdata=testData)
confusionMatrixRF <- confusionMatrix(testData$classe,predictTrain)
# Displaying confusion matrix and accuracy
confusionMatrixRF
plot(RFmodel$finalModel)
```

## Generated Boosted Model (GBM)
```{r message=FALSE}
set.seed(83823)
GBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
GBMmodel  <- train(classe ~ ., data=trainSet, method = "gbm", trControl = GBM, verbose = FALSE)
GBMmodel$finalModel
predictGBM <- predict(GBMmodel, newdata=testData)
confusionMatrixGBM <- confusionMatrix(predictGBM, testData$classe)
confusionMatrixGBM
```


# 6. Conclusion
Classification Tree: 50.01% accuracy
Generalized Boosted: 98.76% accuracy
Random Forest: 99.48% accuracy
Since random forest model has best accuracy, it is used for predictions on the 20 test cases.

```{r message=FALSE}
predictTest <- predict(RFmodel, newdata = TestSet)
predictTest
```
